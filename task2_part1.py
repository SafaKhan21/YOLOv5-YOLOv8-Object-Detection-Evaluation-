# -*- coding: utf-8 -*-
"""task2_part1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ujavNaB8FlCxwY0uRfm9l2fpMapOkgMt
"""

!pip install torch torchvision

!wget https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip
!unzip coco128.zip -d /content/tiny_coco

!pip install torch torchvision
!pip install ultralytics  # For YOLOv5 and YOLOv8
!pip install opencv-python-headless  # For image processing
!pip install matplotlib  # For visualization

import os
import cv2
import time
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
from google.colab.patches import cv2_imshow  # For Colab-specific image display

# Function to calculate Intersection over Union (IoU)
def calculate_iou(box1, box2):
    x_min = max(box1[0], box2[0])
    y_min = max(box1[1], box2[1])
    x_max = min(box1[2], box2[2])
    y_max = min(box1[3], box2[3])
    intersection = max(0, x_max - x_min) * max(0, y_max - y_min)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = box1_area + box2_area - intersection
    return intersection / union if union > 0 else 0

# Function to load ground truth annotations in YOLO format
def load_ground_truth(annotation_path, img_width, img_height):
    ground_truth_data = {}
    for file_name in os.listdir(annotation_path):
        if file_name.endswith('.txt'):
            image_name = os.path.splitext(file_name)[0]
            with open(os.path.join(annotation_path, file_name), 'r') as file:
                annotations = []
                for line in file:
                    cls, x_center, y_center, width, height = map(float, line.strip().split())
                    x_min = (x_center - width / 2) * img_width
                    y_min = (y_center - height / 2) * img_height
                    x_max = (x_center + width / 2) * img_width
                    y_max = (y_center + height / 2) * img_height
                    annotations.append({'class': int(cls), 'bbox': [x_min, y_min, x_max, y_max]})
                ground_truth_data[image_name] = annotations
    return ground_truth_data

# Load YOLOv8 model
model_v8 = YOLO('yolov8s.pt')

# Define paths
image_path = '/content/tiny_coco/coco128/images/train2017'
annotation_path = '/content/tiny_coco/coco128/labels/train2017'

# Set image dimensions
image_width, image_height = 640, 640

# Predict with YOLOv8
results_v8 = model_v8.predict(source=image_path, conf=0.4, save=True)

# Load ground truth data
ground_truth_data = load_ground_truth(annotation_path, image_width, image_height)

iou_scores = []
inference_times = []

# Limit display to 3 images
max_images_to_display = 3
displayed_images = 0

# Loop over predictions and evaluate
for result in results_v8:
    if displayed_images >= max_images_to_display:  # Stop after displaying 3 images
        break

    start_time = time.time()

    # Predicted boxes, scores, and labels
    pred_boxes = result.boxes.xyxy.cpu().numpy()
    pred_labels = result.boxes.cls.cpu().numpy().astype(int)
    pred_scores = result.boxes.conf.cpu().numpy()

    # Extract the image name
    image_name = os.path.splitext(os.path.basename(result.path))[0]

    # Ground truth boxes and labels
    ground_truth_boxes = [item['bbox'] for item in ground_truth_data.get(image_name, [])]
    ground_truth_labels = [item['class'] for item in ground_truth_data.get(image_name, [])]

    # Read image
    img = cv2.imread(result.path)

    # Evaluate IoU for each prediction
    for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):
        best_iou = 0
        for gt_box, gt_label in zip(ground_truth_boxes, ground_truth_labels):
            if pred_label == gt_label:  # Match classes
                iou = calculate_iou(pred_box, gt_box)
                best_iou = max(best_iou, iou)
        if best_iou > 0:
            iou_scores.append(best_iou)

        # Draw predicted box in green
        x_min, y_min, x_max, y_max = map(int, pred_box)
        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
        label = f"{pred_label} ({pred_score:.2f})"
        cv2.putText(img, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Draw ground truth boxes in blue
    for gt_box, gt_label in zip(ground_truth_boxes, ground_truth_labels):
        x_min, y_min, x_max, y_max = map(int, gt_box)
        label = str(gt_label)
        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)
        cv2.putText(img, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    # Calculate inference time
    inference_times.append(time.time() - start_time)

    # Display the image with annotations
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(10, 10))
    plt.imshow(img_rgb)
    plt.title(f"Predictions for {image_name}")
    plt.axis('off')
    plt.show()

    displayed_images += 1  # Increment displayed images count

# IoU Analysis
print("IoU Scores:")
print(iou_scores)
print(f"Average IoU: {np.mean(iou_scores):.4f}")

import os
import numpy as np
import cv2
import time
from ultralytics import YOLO
import matplotlib.pyplot as plt

# Function to calculate Intersection over Union (IoU)
def calculate_iou(box1, box2):
    x_min = max(box1[0], box2[0])
    y_min = max(box1[1], box2[1])
    x_max = min(box1[2], box2[2])
    y_max = min(box1[3], box2[3])
    intersection = max(0, x_max - x_min) * max(0, y_max - y_min)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = box1_area + box2_area - intersection
    return intersection / union if union > 0 else 0

# Function to load ground truth annotations in YOLO format
def load_ground_truth(annotation_path, img_width, img_height):
    ground_truth_data = {}
    for file_name in os.listdir(annotation_path):
        if file_name.endswith('.txt'):
            image_name = os.path.splitext(file_name)[0]
            with open(os.path.join(annotation_path, file_name), 'r') as file:
                annotations = []
                for line in file:
                    cls, x_center, y_center, width, height = map(float, line.strip().split())
                    x_min = (x_center - width / 2) * img_width
                    y_min = (y_center - height / 2) * img_height
                    x_max = (x_center + width / 2) * img_width
                    y_max = (y_center + height / 2) * img_height
                    annotations.append({'class': int(cls), 'bbox': [x_min, y_min, x_max, y_max]})
                ground_truth_data[image_name] = annotations
    return ground_truth_data

# Function to calculate evaluation metrics (precision, recall, F1, accuracy)
def evaluate_model(model, image_path, annotation_path, iou_threshold=0.5, display_limit=3):
    # Load ground truth data
    image_width, image_height = 640, 640
    ground_truth_data = load_ground_truth(annotation_path, image_width, image_height)

    TP = 0  # True Positives
    FP = 0  # False Positives
    FN = 0  # False Negatives

    displayed_images = 0
    results = model.predict(source=image_path, conf=0.4, save=True)  # Perform inference

    # Loop over predictions and calculate metrics
    for result in results:
        if displayed_images >= display_limit:
            break

        image_name = os.path.splitext(os.path.basename(result.path))[0]
        pred_boxes = result.boxes.xyxy.cpu().numpy()
        pred_labels = result.boxes.cls.cpu().numpy().astype(int)
        pred_scores = result.boxes.conf.cpu().numpy()

        ground_truth_boxes = [item['bbox'] for item in ground_truth_data.get(image_name, [])]
        ground_truth_labels = [item['class'] for item in ground_truth_data.get(image_name, [])]

        matched_gt = []
        for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):
            best_iou = 0
            best_gt_idx = -1
            for idx, (gt_box, gt_label) in enumerate(zip(ground_truth_boxes, ground_truth_labels)):
                if pred_label == gt_label:
                    iou = calculate_iou(pred_box, gt_box)
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = idx

            if best_iou >= iou_threshold and best_gt_idx != -1:
                TP += 1
                matched_gt.append(best_gt_idx)
            else:
                FP += 1

        for idx, _ in enumerate(ground_truth_boxes):
            if idx not in matched_gt:
                FN += 1

        displayed_images += 1

    # Calculate metrics
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0

    # Print the results
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1_score:.4f}")
    print(f"Accuracy: {accuracy:.4f}")

    return precision, recall, f1_score, accuracy


# Load YOLOv8 model
model_v8 = YOLO('yolov8s.pt')

# Define paths
image_path = '/content/tiny_coco/coco128/images/train2017'
annotation_path = '/content/tiny_coco/coco128/labels/train2017'

# Call the evaluate function to calculate and display the metrics
evaluate_model(model_v8, image_path, annotation_path)

from ultralytics import YOLO
import cv2
import os
import numpy as np
import time
import matplotlib.pyplot as plt

# Function to calculate Intersection over Union (IoU)
def calculate_iou(box1, box2):
    x_min = max(box1[0], box2[0])
    y_min = max(box1[1], box2[1])
    x_max = min(box1[2], box2[2])
    y_max = min(box1[3], box2[3])

    intersection = max(0, x_max - x_min) * max(0, y_max - y_min)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    union = box1_area + box2_area - intersection
    iou = intersection / union if union > 0 else 0
    return iou

# Function to load ground truth annotations in YOLO format
def load_ground_truth(annotation_path, img_width, img_height):
    ground_truth_data = {}
    for file_name in os.listdir(annotation_path):
        if file_name.endswith('.txt'):
            image_name = os.path.splitext(file_name)[0]
            with open(os.path.join(annotation_path, file_name), 'r') as file:
                annotations = []
                for line in file:
                    cls, x_center, y_center, width, height = map(float, line.strip().split())
                    x_min = (x_center - width / 2) * img_width
                    y_min = (y_center - height / 2) * img_height
                    x_max = (x_center + width / 2) * img_width
                    y_max = (y_center + height / 2) * img_height
                    annotations.append({'class': int(cls), 'bbox': [x_min, y_min, x_max, y_max]})
                ground_truth_data[image_name] = annotations
    return ground_truth_data

# Load YOLOv5 model
model_v5 = YOLO('yolov5s.pt')

# Define paths
image_path = '/content/tiny_coco/coco128/images/train2017'
annotation_path = '/content/tiny_coco/coco128/labels/train2017'

# Set image dimensions
image_width, image_height = 640, 640

# Predict with YOLOv5
results_v5 = model_v5.predict(source=image_path, conf=0.4, save=True)

# Load ground truth data
ground_truth_data = load_ground_truth(annotation_path, image_width, image_height)

# Initialize variables for evaluation
iou_scores = []
inference_times = []
all_gt_labels = []
all_pred_labels = []
image_iou_scores = []

# Track number of displayed images
displayed_images = 0

# Loop over predictions and evaluate
for result in results_v5:
    if displayed_images >= 3:  # Stop after displaying 3 images
        break

    start_time = time.time()

    # Predicted boxes, scores, and labels
    pred_boxes = result.boxes.xyxy.cpu().numpy()
    pred_labels = result.boxes.cls.cpu().numpy().astype(int)
    pred_scores = result.boxes.conf.cpu().numpy()

    # Extract the image name
    image_name = os.path.splitext(os.path.basename(result.path))[0]

    # Ground truth boxes and labels
    ground_truth_boxes = [item['bbox'] for item in ground_truth_data.get(image_name, [])]
    ground_truth_labels = [item['class'] for item in ground_truth_data.get(image_name, [])]

    # Store ground truth and predicted labels
    all_gt_labels.extend(ground_truth_labels)
    all_pred_labels.extend(pred_labels)

    # Read the image
    img = cv2.imread(result.path)

    # Initialize IoU scores for the current image
    current_image_iou = []

    # Evaluate IoU for each prediction
    for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):
        best_iou = 0
        for gt_box, gt_label in zip(ground_truth_boxes, ground_truth_labels):
            if pred_label == gt_label:
                iou = calculate_iou(pred_box, gt_box)
                best_iou = max(best_iou, iou)
        if best_iou > 0:
            iou_scores.append(best_iou)
            current_image_iou.append(best_iou)

        # Draw predicted boxes in green
        x_min, y_min, x_max, y_max = map(int, pred_box)
        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
        label = f"{pred_label} ({pred_score:.2f})"
        cv2.putText(img, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Draw ground truth boxes in blue
    for gt_box, gt_label in zip(ground_truth_boxes, ground_truth_labels):
        x_min, y_min, x_max, y_max = map(int, gt_box)
        label = str(gt_label)
        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)
        cv2.putText(img, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    # Calculate inference time
    inference_times.append(time.time() - start_time)

    # Store IoU for the current image
    image_iou_scores.append(np.mean(current_image_iou) if current_image_iou else 0)

    # Display the IoU for the current image
    print(f"IoU for image {image_name}: {image_iou_scores[-1]:.4f}")

    # Display the image with annotations
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(10, 10))
    plt.imshow(img_rgb)
    plt.title(f"Predictions for {image_name}")
    plt.axis('off')
    plt.show()

    # Increment displayed images counter
    displayed_images += 1

# Calculate and display the average IoU across all images
average_iou = np.mean(iou_scores) if iou_scores else 0
print(f"Average IoU across all images: {average_iou:.4f}")

import os
import numpy as np
import cv2
import time
from ultralytics import YOLO
import matplotlib.pyplot as plt

# Function to calculate Intersection over Union (IoU)
def calculate_iou(box1, box2):
    x_min = max(box1[0], box2[0])
    y_min = max(box1[1], box2[1])
    x_max = min(box1[2], box2[2])
    y_max = min(box1[3], box2[3])
    intersection = max(0, x_max - x_min) * max(0, y_max - y_min)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = box1_area + box2_area - intersection
    return intersection / union if union > 0 else 0

# Function to load ground truth annotations in YOLO format
def load_ground_truth(annotation_path, img_width, img_height):
    ground_truth_data = {}
    for file_name in os.listdir(annotation_path):
        if file_name.endswith('.txt'):
            image_name = os.path.splitext(file_name)[0]
            with open(os.path.join(annotation_path, file_name), 'r') as file:
                annotations = []
                for line in file:
                    cls, x_center, y_center, width, height = map(float, line.strip().split())
                    x_min = (x_center - width / 2) * img_width
                    y_min = (y_center - height / 2) * img_height
                    x_max = (x_center + width / 2) * img_width
                    y_max = (y_center + height / 2) * img_height
                    annotations.append({'class': int(cls), 'bbox': [x_min, y_min, x_max, y_max]})
                ground_truth_data[image_name] = annotations
    return ground_truth_data

# Function to calculate evaluation metrics (precision, recall, F1, accuracy)
def evaluate_model(model, image_path, annotation_path, iou_threshold=0.5, display_limit=3):
    # Load ground truth data
    image_width, image_height = 640, 640
    ground_truth_data = load_ground_truth(annotation_path, image_width, image_height)

    TP = 0  # True Positives
    FP = 0  # False Positives
    FN = 0  # False Negatives

    displayed_images = 0
    results = model.predict(source=image_path, conf=0.4, save=True)  # Perform inference

    # Loop over predictions and calculate metrics
    for result in results:
        if displayed_images >= display_limit:
            break

        image_name = os.path.splitext(os.path.basename(result.path))[0]
        pred_boxes = result.boxes.xyxy.cpu().numpy()
        pred_labels = result.boxes.cls.cpu().numpy().astype(int)
        pred_scores = result.boxes.conf.cpu().numpy()

        ground_truth_boxes = [item['bbox'] for item in ground_truth_data.get(image_name, [])]
        ground_truth_labels = [item['class'] for item in ground_truth_data.get(image_name, [])]

        matched_gt = []
        for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):
            best_iou = 0
            best_gt_idx = -1
            for idx, (gt_box, gt_label) in enumerate(zip(ground_truth_boxes, ground_truth_labels)):
                if pred_label == gt_label:
                    iou = calculate_iou(pred_box, gt_box)
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = idx

            if best_iou >= iou_threshold and best_gt_idx != -1:
                TP += 1
                matched_gt.append(best_gt_idx)
            else:
                FP += 1

        for idx, _ in enumerate(ground_truth_boxes):
            if idx not in matched_gt:
                FN += 1

        displayed_images += 1

    # Calculate metrics
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0

    # Print the results
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1_score:.4f}")
    print(f"Accuracy: {accuracy:.4f}")

    return precision, recall, f1_score, accuracy


# Load YOLOv8 model
model_v8 = YOLO('yolov8s.pt')

# Define paths
image_path = '/content/tiny_coco/coco128/images/train2017'
annotation_path = '/content/tiny_coco/coco128/labels/train2017'

# Call the evaluate function to calculate and display the metrics
evaluate_model(model_v5, image_path, annotation_path)